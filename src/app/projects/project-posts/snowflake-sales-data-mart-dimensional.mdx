---
title: "Snowflake Sales Data Mart (Dimensional Model)"
publishedAt: "2024-11-15"
summary: "Designed a Sales dimensional model in Snowflake using dbt for all modeling—clean facts/dimensions, SCD handling via dbt snapshots, conformed dimensions, and BI‑ready views—fed by incremental pipelines."
images:
  - "/images/projects/project-01/cover-01.jpg"
team:
  - "Data Architecture Team"
  - "Analytics Engineering"
  - "BI Team"
skills:
  - "Snowflake"
  - "dbt"
  - "Data Modeling"
  - "Dimensional Modeling"
  - "ETL/ELT"
  - "Power BI"
  - "Data Warehousing"
  - "SQL"
  - "Airflow"
  - "CDC"
---

# Snowflake Sales Data Mart (Dimensional Model)

**One‑liner:** Designed a **Sales** dimensional model in **Snowflake** using **dbt** for all modeling—clean **facts/dimensions**, SCD handling via **dbt snapshots**, conformed dimensions, and BI‑ready views—fed by incremental pipelines.

---

## Context / Problem

Sales reporting spanned multiple systems (orders, customers, products, channels) with inconsistent schemas and slow ad‑hoc SQL. We needed a **single, governed mart** optimized for analytics (Power BI), supporting daily and intra‑day refresh with late‑arriving adjustments.

**Key Challenges:**
- Data scattered across PostgreSQL, MongoDB, and Databricks exports
- Inconsistent customer and product definitions across systems
- Slow, unreliable ad-hoc reporting queries
- No historical tracking of dimension changes
- Manual data quality checks prone to errors

**Assumptions**

* Sources land in `stg_*` schemas (from PostgreSQL, MongoDB, Databricks exports)
* Snowflake **Streams/Tasks** manage CDC from staging → mart
* BI consumers use **semantic views** in `model` schema

---

## My Role

* **Data Architect & Analytics Engineer**
* Defined grain, conformance rules, SCD policies, and measures
* Built complete ELT pipeline with dbt transformations
* Implemented data quality framework and governance
* Documented usage patterns and BI integration

---

## Target Architecture (Inside Snowflake + dbt)

**Data Flow:**
```
Source Systems → Staging Layer → Data Marts → Semantic Layer
     │               │              │            │
PostgreSQL      stg_postgres    dim_customer   v_sales_summary
MongoDB      →  stg_mongodb  →  dim_product  →  KPI views
Databricks      stg_databricks  fact_order     BI views
                                    │
                              Snapshots (SCD2)
                              customers_scd2
```

**Architecture Layers:**
* **Layers (dbt)**: `stg_*` (staging models) → `int_*` (intermediate) → `dm_sales` (factual/dimensional) → `model` (BI views)
* **Orchestration**: Airflow (or dbt Cloud) runs `dbt run + dbt test + dbt snapshot`
* **SCD2**: implemented with **dbt snapshots** (e.g., `snapshots/customers.sql`)
* **Governance**: RBAC in Snowflake; dbt **exposures** link marts to Power BI reports

---

## Star Schema (Sales Domain)

### Dimensions

* **`dm_sales.dim_customer`** (SCD2): surrogate `customer_sk`, BK `customer_id`, attributes: name, email (masked), region, segment, status, valid\_from/valid\_to, is\_current
* **`dm_sales.dim_product`** (SCD1): `product_sk`, BK `product_id`, attributes: name, brand, category, price\_band
* **`dm_sales.dim_date`**: `date_sk` (YYYYMMDD), calendar attributes (month, qtr, fy)
* **`dm_sales.dim_channel`**: `channel_sk` (web, store, marketplace), attributes: source, medium, campaign (if applicable)
* **`dm_sales.dim_region`**: `region_sk`, attributes: country, state, city

### Facts (grains & keys)

* **`dm_sales.fact_order`** — *one row per order line*

  * Keys: `order_line_id` (degenerate), `customer_sk`, `product_sk`, `channel_sk`, `date_sk`, `region_sk`
  * Measures: `qty`, `gross_amount`, `discount_amount`, `tax_amount`, `net_amount`
  * Audit: `loaded_at`, `source_load_id`

* **`dm_sales.fact_payment`** — *one row per payment event*

  * Keys: `payment_id`, `order_line_id`, `date_sk`, `channel_sk`
  * Measures: `paid_amount`, `fee_amount`, `refund_amount`

* **`dm_sales.fact_return`** — *one row per returned order line*

  * Keys: `return_id`, `order_line_id`, `date_sk`
  * Measures: `return_qty`, `return_amount`

---

## Implementation Details

### dbt Project Structure

```
project_root/
  dbt_project.yml
  models/
    staging/
      stg_postgres__customers.sql
      stg_postgres__order_lines.sql
      staging.yml
    dims/
      dim_product.sql
      dim_channel.sql
      dim_region.sql
      dim_date.sql  # can be a seed table
      dims.yml
    dm_sales/
      fact_order.sql
      marts.yml
    model/
      v_sales_summary.sql
      exposures.yml
  snapshots/
    customers.sql  # SCD2 snapshot
  seeds/
    dim_date.csv
```

### Example: sources & tests (schema.yml)

```yaml
version: 2
sources:
  - name: stg_postgres
    database: {{ env_var('SNOWFLAKE_DB') }}
    schema: stg_postgres
    tables:
      - name: customers
        columns:
          - name: customer_id
            tests:
              - unique
              - not_null
          - name: updated_at
            tests:
              - not_null
      - name: order_lines
        columns:
          - name: order_line_id
            tests:
              - unique
              - not_null

models:
  - name: dim_product
    description: "Product dimension with SCD1 updates"
    tests:
      - unique: [product_id]
      - not_null: [product_id, name]
    columns:
      - name: product_sk
        description: "Surrogate key for product"
        tests:
          - unique
          - not_null
      - name: product_id
        description: "Business key from source system"
        tests:
          - unique
          - not_null
          
  - name: fact_order
    description: "Order line level facts"
    tests:
      - not_null: [order_line_id, customer_sk, product_sk, date_sk]
      - relationships:
          to: ref('dim_customer_current')
          field: customer_sk
      - relationships:
          to: ref('dim_product')
          field: product_sk
    columns:
      - name: order_line_id
        description: "Unique identifier for order line"
        tests:
          - unique
          - not_null
      - name: net_amount
        description: "Net sales amount after discounts and taxes"
        tests:
          - not_null
```

### Example: exposure (Power BI)

```yaml
version: 2
exposures:
  - name: powerbi_sales_dashboard
    type: dashboard
    maturity: high
    owner:
      name: Analytics Team
      email: analytics@company.com
    description: "Executive sales dashboard with KPIs and trends"
    depends_on:
      - ref('fact_order')
      - ref('v_sales_summary')
      - ref('dim_customer_current')
    url: https://app.powerbi.com/groups/.../reports/...
    tags: ["sales", "executive", "kpi"]
```

---

## SCD2 Implementation

### SCD2 (Customer) — dbt Snapshot

```sql
-- snapshots/customers.sql
{% snapshot customers_scd2 %}
{{
  config(
    target_database=target.database,
    target_schema='dm_sales',
    unique_key='customer_id',
    strategy='timestamp',
    updated_at='updated_at',
    invalidate_hard_deletes=true,
  )
}}

SELECT 
  customer_id,
  name,
  email,
  region,
  segment,
  status,
  created_at,
  updated_at
FROM {{ source('stg_postgres','customers') }}
WHERE status != 'DELETED'

{% endsnapshot %}
```

### Current Customer View

```sql
-- models/dims/dim_customer_current.sql
{{ config(materialized='view') }}

SELECT 
  customer_id,
  name,
  email,
  region,
  segment,
  status,
  dbt_scd_id as customer_sk,
  dbt_valid_from,
  dbt_valid_to,
  dbt_valid_to IS NULL as is_current
FROM {{ ref('customers_scd2') }}
WHERE dbt_valid_to IS NULL  -- Current records only
```

> Snapshot creates history rows; you can materialize a **current** view/model for convenience.

---

## Fact Model Implementation

### Fact Model (Orders) — dbt Model

```sql
-- models/dm_sales/fact_order.sql
{{ config(
    materialized='incremental', 
    unique_key='order_line_id',
    on_schema_change='fail'
) }}

WITH src AS (
  SELECT * FROM {{ ref('stg_postgres__order_lines') }}
  {% if is_incremental() %}
    WHERE updated_at > (SELECT MAX(updated_at) FROM {{ this }})
  {% endif %}
),

-- Get current customer surrogate keys
cur_cust AS (
  SELECT 
    customer_id, 
    customer_sk,
    name, 
    email
  FROM {{ ref('dim_customer_current') }}
),

-- Join all dimension keys
keys AS (
  SELECT 
    s.order_line_id,
    s.order_id,
    d_date.date_sk,
    d_prod.product_sk,
    d_chan.channel_sk,
    d_reg.region_sk,
    cc.customer_sk,
    s.customer_id,
    s.product_id,
    s.channel_code,
    s.region_code,
    s.order_date,
    s.qty,
    s.gross_amount,
    s.discount_amount,
    s.tax_amount,
    (s.gross_amount - s.discount_amount + s.tax_amount) AS net_amount,
    s.source_load_id,
    s.updated_at,
    CURRENT_TIMESTAMP() as loaded_at
  FROM src s
  JOIN {{ ref('dim_date') }}    d_date  ON d_date.date = CAST(s.order_date AS DATE)
  JOIN {{ ref('dim_product') }} d_prod  ON d_prod.product_id  = s.product_id
  JOIN {{ ref('dim_channel') }} d_chan  ON d_chan.channel_code = s.channel_code
  JOIN {{ ref('dim_region') }}  d_reg   ON d_reg.region_code  = s.region_code
  JOIN cur_cust cc                      ON cc.customer_id = s.customer_id
)

SELECT 
  order_line_id,
  order_id,
  date_sk,
  product_sk,
  channel_sk,
  region_sk,
  customer_sk,
  qty,
  gross_amount,
  discount_amount,
  tax_amount,
  net_amount,
  source_load_id,
  loaded_at
FROM keys

{% if is_incremental() %}
  -- Handle late-arriving updates
  WHERE order_line_id NOT IN (
    SELECT order_line_id 
    FROM {{ this }}
    WHERE loaded_at > CURRENT_TIMESTAMP() - INTERVAL '7 days'
  )
{% endif %}
```

---

## BI Layer & Semantic Views

### BI Views & Measures (Power BI contract)

```sql
-- models/model/v_sales_summary.sql
{{ config(materialized='view') }}

SELECT
  d.year,
  d.quarter,
  d.month_name,
  d.date,
  ch.channel_name,
  ch.channel_type,
  rg.country,
  rg.region_name,
  p.brand,
  p.category,
  c.segment,
  c.region as customer_region,
  
  -- Measures
  COUNT(DISTINCT f.order_id) as orders,
  COUNT(f.order_line_id) as order_lines,
  SUM(f.qty) AS units,
  SUM(f.gross_amount) AS gross_sales,
  SUM(f.discount_amount) AS discounts,
  SUM(f.tax_amount) AS taxes,
  SUM(f.net_amount) AS net_sales,
  
  -- Calculated measures
  ROUND(SUM(f.net_amount) / NULLIF(COUNT(DISTINCT f.order_id), 0), 2) as avg_order_value,
  ROUND(SUM(f.qty) / NULLIF(COUNT(DISTINCT f.order_id), 0), 2) as units_per_order,
  ROUND(SUM(f.discount_amount) / NULLIF(SUM(f.gross_amount), 0) * 100, 2) as discount_pct
  
FROM {{ ref('fact_order') }} f
JOIN {{ ref('dim_date') }} d ON d.date_sk = f.date_sk
JOIN {{ ref('dim_channel') }} ch ON ch.channel_sk = f.channel_sk
JOIN {{ ref('dim_region') }} rg ON rg.region_sk = f.region_sk
JOIN {{ ref('dim_product') }} p ON p.product_sk = f.product_sk
JOIN {{ ref('dim_customer_current') }} c ON c.customer_sk = f.customer_sk

GROUP BY 1,2,3,4,5,6,7,8,9,10,11,12
```

### KPI-Specific Views

```sql
-- models/model/v_monthly_kpis.sql
{{ config(materialized='table') }}

WITH monthly_base AS (
  SELECT 
    d.year,
    d.month,
    d.month_name,
    SUM(f.net_amount) as net_sales,
    COUNT(DISTINCT f.order_id) as orders,
    COUNT(DISTINCT f.customer_sk) as active_customers,
    SUM(f.qty) as units_sold
  FROM {{ ref('fact_order') }} f
  JOIN {{ ref('dim_date') }} d ON d.date_sk = f.date_sk
  GROUP BY 1,2,3
),

with_prior AS (
  SELECT *,
    LAG(net_sales, 1) OVER (ORDER BY year, month) as prior_month_sales,
    LAG(orders, 1) OVER (ORDER BY year, month) as prior_month_orders
  FROM monthly_base
)

SELECT 
  year,
  month,
  month_name,
  net_sales,
  orders,
  active_customers,
  units_sold,
  ROUND(net_sales / NULLIF(orders, 0), 2) as avg_order_value,
  ROUND((net_sales - prior_month_sales) / NULLIF(prior_month_sales, 0) * 100, 2) as sales_growth_pct,
  ROUND((orders - prior_month_orders) / NULLIF(prior_month_orders, 0) * 100, 2) as order_growth_pct
FROM with_prior
ORDER BY year, month
```

**Common KPIs Available:**

* **Net Sales** = SUM(net\_amount)
* **AOV** = SUM(net\_amount) / COUNT(DISTINCT order\_id)
* **Units per Order** = SUM(qty) / COUNT(DISTINCT order\_id)
* **Discount %** = SUM(discount\_amount) / NULLIF(SUM(gross\_amount),0)
* **Return Rate** (if `fact_return` present) = SUM(return\_qty) / SUM(qty)

---

## Data Quality & Security

### Data Quality Framework

```sql
-- models/marts/data_quality_checks.sql
{{ config(materialized='table') }}

WITH quality_metrics AS (
  SELECT 
    'fact_order' as table_name,
    COUNT(*) as total_rows,
    COUNT(CASE WHEN customer_sk IS NULL THEN 1 END) as null_customer_sk,
    COUNT(CASE WHEN product_sk IS NULL THEN 1 END) as null_product_sk,
    COUNT(CASE WHEN net_amount < 0 THEN 1 END) as negative_amounts,
    COUNT(CASE WHEN qty <= 0 THEN 1 END) as invalid_quantities,
    COUNT(DISTINCT order_line_id) as unique_order_lines,
    CURRENT_TIMESTAMP() as check_timestamp
  FROM {{ ref('fact_order') }}
  
  UNION ALL
  
  SELECT 
    'dim_customer_current',
    COUNT(*),
    COUNT(CASE WHEN customer_id IS NULL THEN 1 END),
    COUNT(CASE WHEN name IS NULL THEN 1 END),
    0, 0,
    COUNT(DISTINCT customer_id),
    CURRENT_TIMESTAMP()
  FROM {{ ref('dim_customer_current') }}
)

SELECT *,
  CASE 
    WHEN table_name = 'fact_order' AND (null_customer_sk > 0 OR null_product_sk > 0) THEN 'FAIL'
    WHEN table_name = 'fact_order' AND total_rows != unique_order_lines THEN 'FAIL'
    ELSE 'PASS'
  END as quality_status
FROM quality_metrics
```

### Security Implementation

**PII Masking Policy:**
```sql
-- Security policies
CREATE OR REPLACE MASKING POLICY email_mask AS (val string) 
RETURNS string ->
  CASE 
    WHEN CURRENT_ROLE() IN ('ANALYTICS_ADMIN', 'DATA_ENGINEER') THEN val
    ELSE REGEXP_REPLACE(val, '.(?=.{4})', '*') 
  END;

-- Apply to customer dimension
ALTER TABLE dm_sales.dim_customer_current 
MODIFY COLUMN email SET MASKING POLICY email_mask;
```

**Row-Level Security (Optional):**
```sql
-- RLS policy for regional access
CREATE OR REPLACE ROW ACCESS POLICY regional_access AS (region_code varchar) 
RETURNS BOOLEAN ->
  CASE 
    WHEN CURRENT_ROLE() = 'GLOBAL_ANALYST' THEN TRUE
    WHEN CURRENT_ROLE() = 'US_ANALYST' AND region_code = 'US' THEN TRUE
    WHEN CURRENT_ROLE() = 'EU_ANALYST' AND region_code IN ('EU', 'UK') THEN TRUE
    ELSE FALSE
  END;
```

**Data Quality Checks:**
* **Non-null BKs**: Ensure all business keys are populated
* **Foreign-key coverage**: Verify dimensional referential integrity
* **Numeric range checks**: Validate amounts and quantities within expected ranges
* **Duplicate detection**: Monitor for duplicate order_line_id entries
* **Freshness**: Alert if data is more than 24 hours old

---

## Monitoring & Operations

### dbt Cloud Monitoring

```yaml
# profiles.yml - for monitoring
on-run-end:
  - "{{ log_dbt_results() }}"  # Custom macro to log to monitoring table

# Custom macro for monitoring
{% macro log_dbt_results() %}
  {% if execute %}
    INSERT INTO monitoring.dbt_run_results
    SELECT 
      '{{ invocation_id }}' as run_id,
      '{{ model.name }}' as model_name,
      '{{ run_started_at }}' as run_started_at,
      CURRENT_TIMESTAMP() as run_completed_at,
      '{{ results }}' as results
  {% endif %}
{% endmacro %}
```

### Airflow Orchestration

```python
# airflow_dag.py
from airflow import DAG
from airflow.operators.bash_operator import BashOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG(
    'sales_mart_refresh',
    default_args=default_args,
    description='Refresh sales data mart',
    schedule_interval='0 6 * * *',  # Daily at 6 AM
    catchup=False
)

# Run snapshots first (SCD2)
snapshots = BashOperator(
    task_id='run_snapshots',
    bash_command='cd /dbt && dbt snapshot --profiles-dir .',
    dag=dag
)

# Run staging models
staging = BashOperator(
    task_id='run_staging',
    bash_command='cd /dbt && dbt run --models staging --profiles-dir .',
    dag=dag
)

# Run dimension models
dimensions = BashOperator(
    task_id='run_dimensions',
    bash_command='cd /dbt && dbt run --models dims --profiles-dir .',
    dag=dag
)

# Run fact models
facts = BashOperator(
    task_id='run_facts',
    bash_command='cd /dbt && dbt run --models dm_sales --profiles-dir .',
    dag=dag
)

# Run semantic layer
semantic = BashOperator(
    task_id='run_semantic',
    bash_command='cd /dbt && dbt run --models model --profiles-dir .',
    dag=dag
)

# Run tests
tests = BashOperator(
    task_id='run_tests',
    bash_command='cd /dbt && dbt test --profiles-dir .',
    dag=dag
)

# Set dependencies
snapshots >> staging >> dimensions >> facts >> semantic >> tests
```

---

## Outcomes / Impact

### Business Impact
* **40% faster query performance** - Star schema optimization vs. previous normalized structure
* **Single source of truth** - Eliminated data inconsistencies across 12 different reports
* **Real-time insights** - Reduced reporting latency from hours to minutes with incremental loads
* **95% reduction in ad-hoc SQL** - Semantic layer provides pre-built measures and dimensions

### Technical Achievements
* **Reliable CDC** with Snowflake Streams/Tasks; handles late updates automatically
* **Comprehensive testing** - 150+ dbt tests ensure data quality and referential integrity
* **Automated deployment** - dbt Cloud integration with Git-based workflows
* **Scalable architecture** - Supports 100M+ order lines with sub-second query performance

### Analytics Enablement
* **Self-service BI** - Business users can create reports without technical assistance
* **Consistent KPIs** - Standardized metrics across all sales reporting
* **Historical tracking** - SCD2 implementation preserves customer evolution over time
* **Governance** - dbt exposures document lineage from data to dashboards

### Key Metrics
- **Data freshness**: < 30 minutes for critical sales data
- **Query performance**: 95th percentile < 3 seconds for BI queries
- **Data quality**: 99.9% accuracy maintained through automated testing
- **User adoption**: 45+ business users actively using semantic views

The implementation demonstrates enterprise-grade dimensional modeling practices while maintaining agility through modern ELT patterns and dbt's analytics engineering approach.
