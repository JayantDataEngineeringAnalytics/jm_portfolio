---
title: "PySpark Refactor → Unity Catalog‑native Batch & Streaming"
publishedAt: "2024-09-15"
summary: "Refactored legacy PySpark pipelines to leverage Unity Catalog features end‑to‑end—Auto Loader (cloudFiles), Delta schema evolution/enforcement, managed tables, lineage, and policy‑based security—across both batch and streaming jobs."
skills: ["PySpark", "Databricks", "Unity Catalog", "Delta Lake", "Auto Loader", "Azure", "Data Governance", "Streaming", "Power BI", "Data Architecture"]
---

Refactored legacy PySpark pipelines to leverage Unity Catalog features end‑to‑end—Auto Loader (cloudFiles), Delta schema evolution/enforcement, managed tables, lineage, and policy‑based security—across both batch and streaming jobs.

---

## Context / Problem

Existing ingestion used hand‑rolled directory scans, manual schema handling, and path‑based writes. This caused brittle pipelines, late arrivals, and frequent rebuilds when source schemas changed. We needed:

* Declarative ingestion with automatic file discovery & checkpoints
* Centralized governance (grants, masking/RLS) under Unity Catalog
* Managed storage semantics (no direct paths) for curated layers
* A single pattern usable for batch and streaming use cases
* Clear lineage from notebooks/SQL to downstream BI

Assumptions (no client names):

* Cloud: Azure
* Storage topology: one ADLS Gen2 account; containers per layer (`raw`, `working`, `silver`, `gold`)
* Curated data: UC‑managed Delta tables
* Ingestion landing: `raw` container (external paths ok for landing)
* Orchestration: Databricks Jobs (optional ADF triggers)
* BI: Power BI via SQL Warehouse

---

## My Role

* Data Architect & Lead Engineer (4 engineers)
* Defined UC‑native patterns, built reference implementations, led rollout
* Authored validation harness & runbooks; coached team on UC grants

---

## Architecture (Target)

* Landing (raw): external paths ingested via Auto Loader (cloudFiles) into bronze managed tables
* Curation (silver/gold): UC managed Delta tables with constraints, expectations, and CDC merges
* Governance: UC grants (group‑based), masking policies, optional dynamic views for RLS
* Lineage: UC lineage across notebooks/SQL/PBI models
* Non‑tabular assets: UC Volumes for checkpoints/artifacts where appropriate
* Access: service principals + SCIM group sync

Flow: Source files → Auto Loader → Bronze (managed) → Silver transforms → Gold marts → SQL Warehouse (PBI)

---

## Key Decisions & Patterns

* Three‑part names everywhere: `catalog.schema.table`
* Auto Loader for both streaming and "micro‑batch" (using `trigger(availableNow=True)` for catch‑up batch)
* Managed tables for curated layers; path writes only in landing
* Schema evolution via `cloudFiles.schemaEvolutionMode = addNewColumns` with explicit governance
* Idempotent merges for UPSERT logic and late arrivals
* Grants as code (idempotent SQL) bound to groups, never users
* Testing harness for counts/checksums/SLAs vs legacy jobs

---

## Refactor Plan

1. Inventory & Classify existing jobs (batch vs streaming, sources, SLAs)
2. Enable UC on workspaces; set default catalog; attach to UC metastore
3. Create managed locations at catalog/schema; define bronze/silver/gold schemas
4. Replace directory scans with Auto Loader (file notification or directory listing)
5. Standardize write semantics: `saveAsTable` (no `LOCATION`) for curated tables
6. Add schema governance: expectations, constraints, and evolution rules
7. Harden merge logic for dimension/fact upserts; dedupe on business keys
8. Introduce grants & policies: SELECT roles, masking on PII, RLS views where needed
9. Cutover: dual‑run, validate, switch downstream to UC tables, retire legacy paths

---

## Snippets (Illustrative)

### 1) Auto Loader — Streaming Bronze (PySpark)

```python
from pyspark.sql.functions import *

raw_path = "abfss://raw@<acct>.dfs.core.windows.net/sales/orders/"
bronze_tbl = "sales_bronze.orders"  # catalog.schema.table

(df := (spark.readStream
  .format("cloudFiles")
  .option("cloudFiles.format", "json")
  .option("cloudFiles.schemaLocation", "/Volumes/platform/checkpoints/orders_schema")
  .option("cloudFiles.schemaEvolutionMode", "addNewColumns")
  .load(raw_path))) \
  .withColumn("ingest_ts", current_timestamp()) \
  .writeStream \
  .format("delta") \
  .option("checkpointLocation", "/Volumes/platform/checkpoints/orders_chkpt") \
  .outputMode("append") \
  .toTable(bronze_tbl)
```

### 2) Auto Loader — Batch Catch‑up (Available Now)

```python
from pyspark.sql.functions import *

raw_path = "abfss://raw@<acct>.dfs.core.windows.net/sales/customers/"
bronze_tbl = "sales_bronze.customers"

spark.readStream \
  .format("cloudFiles") \
  .option("cloudFiles.format", "csv") \
  .option("cloudFiles.inferColumnTypes", "true") \
  .option("cloudFiles.schemaLocation", "/Volumes/platform/checkpoints/customers_schema") \
  .load(raw_path) \
  .writeStream \
  .trigger(availableNow=True) \
  .option("checkpointLocation", "/Volumes/platform/checkpoints/customers_chkpt") \
  .toTable(bronze_tbl)
```

### 3) Silver Merge (UPSERT with late arrivals)

```sql
MERGE INTO sales_silver.orders AS tgt
USING (
  SELECT *, to_date(order_ts) AS order_date
  FROM sales_bronze.orders
) AS src
ON tgt.order_id = src.order_id
WHEN MATCHED THEN UPDATE SET *
WHEN NOT MATCHED THEN INSERT *;
```

### 4) Managed Schema Setup (SQL)

```sql
CREATE CATALOG IF NOT EXISTS sales
  MANAGED LOCATION 'abfss://uc-managed@<acct>.dfs.core.windows.net/sales';

CREATE SCHEMA IF NOT EXISTS sales_bronze;  -- inherits managed location
CREATE SCHEMA IF NOT EXISTS sales_silver;
CREATE SCHEMA IF NOT EXISTS sales_gold;
```

### 5) Masking & RLS (SQL)

```sql
CREATE MASKING POLICY mask_email
  AS (email STRING) -> CASE
    WHEN is_account_group_member('grp_piisafe') THEN email
    ELSE regexp_replace(email, '(^.).*(@.*$)', '$1***$2') END;

ALTER TABLE sales_silver.customers
  ALTER COLUMN email SET MASKING POLICY mask_email;

CREATE OR REPLACE VIEW sales_gold.orders_rls AS
SELECT * FROM sales_gold.orders o
WHERE (
  is_account_group_member('grp_region_apac') AND o.region = 'APAC'
) OR (
  is_account_group_member('grp_region_emea') AND o.region = 'EMEA'
) OR is_account_group_member('grp_admin');
```

---

## Testing & Validation

* Structural: object counts, schema parity, partitioning
* Data: row counts, min/max per partition, checksums, dup checks
* Performance: end‑to‑end latency (streaming), batch wall time, Auto Loader file backlog burn‑down
* Security: grant enforcement, masking/RLS impersonation tests
* BI: PBIX refresh parity via SQL Warehouse

---

## Outcomes / Impact

* Resilience: Auto Loader eliminated missed files & fragile directory scans
* Velocity: schema evolution reduced engineering effort on source changes
* Governance: UC managed tables + policies simplified audits and access
* Performance: lower end‑to‑end latency for streaming, faster batch catch‑ups
* Developer Experience: standardized patterns, fewer bespoke scripts

---

## Risks & Mitigations

* Schema drift abuse → require approval for evolution to gold; monitor new columns via alerts
* Checkpoint hygiene → relocate to UC Volumes and rotate on major schema changes
* Merge skew → enforce partitioning & Z‑ORDER, selective columns in UPDATE
* Backfill load → use `availableNow` with tuned `maxFilesPerTrigger` and autoscaling
